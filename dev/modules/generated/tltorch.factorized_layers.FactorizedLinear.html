<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>tltorch.factorized_layers.FactorizedLinear &#8212; TensorLy-Torch 0.4.0 documentation</title> 
<link rel="stylesheet" href="../../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/tensorly_style.css?v=a02e9698" />

  
    <script src="../../_static/documentation_options.js?v=6c02275b"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
 <script src="../../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3V91QCZR03"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QSPLEF75VT');
</script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="tltorch.factorized_layers.FactorizedConv" href="tltorch.factorized_layers.FactorizedConv.html" />
    <link rel="prev" title="tltorch.factorized_layers.TCL" href="tltorch.factorized_layers.TCL.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../../index.html">
            <img src="../../_static/tensorly-torch-logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/tensorly/torch" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../../install.html">
              Install
            </a>
              <a class="navbar-item" href="../../user_guide/index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../api.html">
              API
            </a>
              <a class="navbar-item" href="../../about.html">
              About Us
            </a>
            <div class="navbar-item has-dropdown is-hoverable is-boxed">
              <a class="navbar-link">
                Ecosystem
              </a>
              <div class="navbar-dropdown top-navbar">
                <a class="navbar-item" href="http://tensorly.org/dev" target="_blank">
                  TensorLy
                </a>
                <a class="navbar-item" href="http://tensorly.org/viz" target="_blank">
                  TensorLy-Viz
                </a>
                <a class="navbar-item" href="http://tensorly.org/quantum" target="_blank">
                  TensorLy-Quantum
                </a>
              </div>
            </div>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/tensorly/torch" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  
      <div class="column is-10-mobile is-one-third-tablet is-3-desktop is-hidden-mobile" id="sidebar">
    
    <aside class="sticky-nav sidebar-menu">
<div class="sidebar-search">
  <form class="field" id="searchbox" role="search" action="../../search.html" method="get">
    <!-- <label class="label" id="searchlabel">Quick search</label> -->
    <div class="field has-addons">
      <div class="control is-expanded">
        <input class="input" type="text" placeholder="Search TensorLy-Torch" name="q" aria-labelledby="searchlabel autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      </div>
      <div class="control">
        <input class="button is-info" type="submit" value="Go" />
      </div>
    </div>
  </form>
  <script>document.getElementById('searchbox').style.display = "block"</script>

</div>
      
      <div class="sidebar-menu-toc">
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installing tensorly-Torch</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api.html#factorized-tensors">Factorized Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#tensorized-matrices">Tensorized Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#complex-tensors">Complex Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#module-tltorch.factorized_tensors">Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#module-tltorch.factorized_layers">Tensor Regression Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#tensor-contraction-layers">Tensor Contraction Layers</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api.html#factorized-linear-layers">Factorized Linear Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#factorized-convolutions">Factorized Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#factorized-embeddings">Factorized Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#module-tltorch.tensor_hooks">Tensor Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#l1-regularization">L1 Regularization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#utilities">Utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/index.html">User guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev_guide/index.html">Development guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About Us</a></li>
</ul>
 
      </div>
    </aside>
  </div>
  

  <div class="column main-column">

    
    <div class="main-section">

      
      
      <div class="side-menu-toggle">
        <button class="button" id="toggle-sidebar" onclick="toggle_sidebar()">
          <span class="icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
          <span>menu</span> 
        </button>
      </div>
      

      <div class="container content main-content">
        
  <section id="tltorch-factorized-layers-factorizedlinear">
<h1><a class="reference internal" href="../api.html#module-tltorch.factorized_layers" title="tltorch.factorized_layers"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tltorch.factorized_layers</span></code></a>.FactorizedLinear<a class="headerlink" href="#tltorch-factorized-layers-factorizedlinear" title="Link to this heading">Â¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="tltorch.factorized_layers.FactorizedLinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tltorch.factorized_layers.</span></span><span class="sig-name descname"><span class="pre">FactorizedLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_tensorized_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_tensorized_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factorization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cp'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">implementation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'factorized'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpointing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tltorch/factorized_layers/factorized_linear.html#FactorizedLinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tltorch.factorized_layers.FactorizedLinear" title="Link to this definition">Â¶</a></dt>
<dd><p>Tensorized Fully-Connected Layers</p>
<p>The weight matrice is tensorized to a tensor of size <cite>(*in_tensorized_features, *out_tensorized_features)</cite>.
That tensor is expressed as a low-rank tensor.</p>
<p>During inference, the full tensor is reconstructed, and unfolded back into a matrix, 
used for the forward pass in a regular linear layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>in_tensorized_features</strong><span class="classifier">int tuple</span></dt><dd><p>shape to which the input_features dimension is tensorized to
e.g. if in_features is 8 in_tensorized_features could be (2, 2, 2)
should verify prod(in_tensorized_features) = in_features</p>
</dd>
<dt><strong>out_tensorized_features</strong><span class="classifier">int tuple</span></dt><dd><p>shape to which the input_features dimension is tensorized to.</p>
</dd>
<dt><strong>factorization</strong><span class="classifier">str, default is âcpâ</span></dt><dd></dd>
<dt><strong>rank</strong><span class="classifier">int tuple or str</span></dt><dd></dd>
<dt><strong>implementation</strong><span class="classifier">{âfactorizedâ, âreconstructedâ}, default is âfactorizedâ</span></dt><dd><p>which implementation to use for forward function:
- if âfactorizedâ, will directly contract the input with the factors of the decomposition
- if âreconstructedâ, the full weight matrix is reconstructed from the factorized version and used for a regular linear layer forward pass.</p>
</dd>
<dt><strong>n_layers</strong><span class="classifier">int, default is 1</span></dt><dd><p>number of linear layers to be parametrized with a single factorized tensor</p>
</dd>
<dt><strong>bias</strong><span class="classifier">bool, default is True</span></dt><dd></dd>
<dt><strong>checkpointing</strong><span class="classifier">bool</span></dt><dd><p>whether to enable gradient checkpointing to save memory during training-mode forward, default is False</p>
</dd>
<dt><strong>device</strong><span class="classifier">PyTorch device to use, default is None</span></dt><dd></dd>
<dt><strong>dtype</strong><span class="classifier">PyTorch dtype, default is None</span></dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#tltorch.factorized_layers.FactorizedLinear.forward" title="tltorch.factorized_layers.FactorizedLinear.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(x[,Â indices])</p></td>
<td><p>Define the computation performed at every call.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#tltorch.factorized_layers.FactorizedLinear.from_linear" title="tltorch.factorized_layers.FactorizedLinear.from_linear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_linear</span></code></a>(linear[,Â rank,Â auto_tensorize,Â ...])</p></td>
<td><p>Class method to create an instance from an existing linear layer</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#tltorch.factorized_layers.FactorizedLinear.from_linear_list" title="tltorch.factorized_layers.FactorizedLinear.from_linear_list"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_linear_list</span></code></a>(linear_list,Â ...[,Â bias,Â ...])</p></td>
<td><p>Class method to create an instance from an existing linear layer</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><strong>get_linear</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>reset_parameters</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="tltorch.factorized_layers.FactorizedLinear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tltorch/factorized_layers/factorized_linear.html#FactorizedLinear.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tltorch.factorized_layers.FactorizedLinear.forward" title="Link to this definition">Â¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tltorch.factorized_layers.FactorizedLinear.from_linear">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">linear</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_tensorize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_tensorized_modes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_tensorized_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_tensorized_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factorization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CP'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">implementation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'reconstructed'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpointing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decomposition_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tltorch/factorized_layers/factorized_linear.html#FactorizedLinear.from_linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tltorch.factorized_layers.FactorizedLinear.from_linear" title="Link to this definition">Â¶</a></dt>
<dd><p>Class method to create an instance from an existing linear layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>linear</strong><span class="classifier">torch.nn.Linear</span></dt><dd><p>layer to tensorize</p>
</dd>
<dt><strong>auto_tensorize</strong><span class="classifier">bool, default is True</span></dt><dd><p>if True, automatically find values for the tensorized_shapes</p>
</dd>
<dt><strong>n_tensorized_modes</strong><span class="classifier">int, default is 3</span></dt><dd><p>Order (number of dims) of the tensorized weights if auto_tensorize is True</p>
</dd>
<dt><strong>in_tensorized_features, out_tensorized_features</strong><span class="classifier">tuple</span></dt><dd><p>shape to tensorized the factorized_weight matrix to.
Must verify np.prod(tensorized_shape) == np.prod(linear.factorized_weight.shape)</p>
</dd>
<dt><strong>factorization</strong><span class="classifier">str, default is âcpâ</span></dt><dd></dd>
<dt><strong>implementation</strong><span class="classifier">str</span></dt><dd><p>which implementation to use for forward function. support âfactorizedâ and âreconstructedâ, default is âfactorizedâ</p>
</dd>
<dt><strong>checkpointing</strong><span class="classifier">bool</span></dt><dd><p>whether to enable gradient checkpointing to save memory during training-mode forward, default is False</p>
</dd>
<dt><strong>rank</strong><span class="classifier">{rank of the decomposition, âsameâ, float}</span></dt><dd><p>if float, percentage of parameters of the original factorized_weights to use
if âsameâ use the same number of parameters</p>
</dd>
<dt><strong>bias</strong><span class="classifier">bool, default is True</span></dt><dd></dd>
<dt><strong>verbose</strong><span class="classifier">bool, default is False</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tltorch.factorized_layers.FactorizedLinear.from_linear_list">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_linear_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">linear_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_tensorized_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_tensorized_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factorization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CP'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">implementation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'reconstructed'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpointing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decomposition_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{'init':</span> <span class="pre">'random'}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tltorch/factorized_layers/factorized_linear.html#FactorizedLinear.from_linear_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#tltorch.factorized_layers.FactorizedLinear.from_linear_list" title="Link to this definition">Â¶</a></dt>
<dd><p>Class method to create an instance from an existing linear layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>linear</strong><span class="classifier">torch.nn.Linear</span></dt><dd><p>layer to tensorize</p>
</dd>
<dt><strong>tensorized_shape</strong><span class="classifier">tuple</span></dt><dd><p>shape to tensorized the weight matrix to.
Must verify np.prod(tensorized_shape) == np.prod(linear.weight.shape)</p>
</dd>
<dt><strong>factorization</strong><span class="classifier">str, default is âcpâ</span></dt><dd></dd>
<dt><strong>implementation</strong><span class="classifier">str</span></dt><dd><p>which implementation to use for forward function. support âfactorizedâ and âreconstructedâ, default is âfactorizedâ</p>
</dd>
<dt><strong>checkpointing</strong><span class="classifier">bool</span></dt><dd><p>whether to enable gradient checkpointing to save memory during training-mode forward, default is False</p>
</dd>
<dt><strong>rank</strong><span class="classifier">{rank of the decomposition, âsameâ, float}</span></dt><dd><p>if float, percentage of parameters of the original weights to use
if âsameâ use the same number of parameters</p>
</dd>
<dt><strong>bias</strong><span class="classifier">bool, default is True</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<div class="clearer"></div></section>


      </div>

      
        <nav class="pagination" role="navigation" aria-label="pagination">
    
    <a class="button pagination-previous" href="tltorch.factorized_layers.TCL.html" title="previous page" accesskey="p">
        <span class="icon">
            <i class="fa fa-arrow-circle-left"></i>
        </span>
        <span><code class="xref py py-mod docutils literal notranslate"><span class="pre">tltorch.factorized_layers</span></code>.TCL</span>
    </a>
    
    
    <a class="button pagination-next" href="tltorch.factorized_layers.FactorizedConv.html" title="next page" accesskey="n">
        <span><code class="xref py py-mod docutils literal notranslate"><span class="pre">tltorch.factorized_layers</span></code>.FactorizedConv </span>
        <span class="icon">
            <i class="fa fa-arrow-circle-right"></i>
        </span>
    </a>
    
</nav>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2024, Jean Kossaifi.<br/>
        </div>
      <div class="block">
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> and the <a href="http://tensorly.org"><strong>TensorLy</strong></a> theme by <a href="http://jeankossaifi.com">Jean Kossaifi</a>.
      </div>
    </div>
  </footer>

    </div>

  </div>  

	
    
    <div class="column is-hidden-touch is-2-desktop is-one-fifth-widescreen" id="localtoc-column">

    <aside class="sticky-nav localtoc"> 
        <p class="menu-label"> 
            <span class="icon-text">
                <span class="icon"><i class="fas fa-duotone fa-list"></i></span>
                <span> On this page </span>
            </span>
        </p>

        <div class="menu menu-list localtoc-list">
        <ul>
<li><a class="reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tltorch.factorized_layers</span></code>.FactorizedLinear</a><ul>
<li><a class="reference internal" href="#tltorch.factorized_layers.FactorizedLinear"><code class="docutils literal notranslate"><span class="pre">FactorizedLinear</span></code></a><ul>
<li><a class="reference internal" href="#tltorch.factorized_layers.FactorizedLinear.forward"><code class="docutils literal notranslate"><span class="pre">FactorizedLinear.forward()</span></code></a></li>
<li><a class="reference internal" href="#tltorch.factorized_layers.FactorizedLinear.from_linear"><code class="docutils literal notranslate"><span class="pre">FactorizedLinear.from_linear()</span></code></a></li>
<li><a class="reference internal" href="#tltorch.factorized_layers.FactorizedLinear.from_linear_list"><code class="docutils literal notranslate"><span class="pre">FactorizedLinear.from_linear_list()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
    </aside>
    </div>

  

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>