
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>tltorch.factorized_layers.FactorizedEmbedding &#8212; TensorLy-Torch 0.4.0 documentation</title> 
<link rel="stylesheet" href="../../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tensorly_style.css" />

  
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
 <script src="../../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3V91QCZR03"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QSPLEF75VT');
</script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="tltorch.tensor_hooks.tensor_dropout" href="tltorch.tensor_hooks.tensor_dropout.html" />
    <link rel="prev" title="tltorch.factorized_layers.FactorizedConv" href="tltorch.factorized_layers.FactorizedConv.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../../index.html">
            <img src="../../_static/tensorly-torch-logo.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/tensorly/torch" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../../install.html">
              Install
            </a>
              <a class="navbar-item" href="../../user_guide/index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../api.html">
              API
            </a>
              <a class="navbar-item" href="../../about.html">
              About Us
            </a>
            <div class="navbar-item has-dropdown is-hoverable is-boxed">
              <a class="navbar-link">
                Ecosystem
              </a>
              <div class="navbar-dropdown top-navbar">
                <a class="navbar-item" href="http://tensorly.org/dev" target="_blank">
                  TensorLy
                </a>
                <a class="navbar-item" href="http://tensorly.org/viz" target="_blank">
                  TensorLy-Viz
                </a>
                <a class="navbar-item" href="http://tensorly.org/quantum" target="_blank">
                  TensorLy-Quantum
                </a>
              </div>
            </div>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/tensorly/torch" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  
      <div class="column is-10-mobile is-one-third-tablet is-3-desktop is-hidden-mobile" id="sidebar">
    
    <aside class="sticky-nav sidebar-menu">
<div class="sidebar-search">
  <form class="field" id="searchbox" role="search" action="../../search.html" method="get">
    <!-- <label class="label" id="searchlabel">Quick search</label> -->
    <div class="field has-addons">
      <div class="control is-expanded">
        <input class="input" type="text" placeholder="Search TensorLy-Torch" name="q" aria-labelledby="searchlabel">
      </div>
      <div class="control">
        <input class="button is-info" type="submit" value="Go" />
      </div>
    </div>
  </form>
  <script>$('#searchbox').show(0);</script>
  <script>
  $(document).ready(function() {
    Document.highlightSearchWords = function() {
      var params = $.getQueryParameters();
      var terms = (params.highlight) ? params.highlight[0].split(/\s+/) : [];
      if (terms.length) {
        var body = $('div.body');
        if (!body.length) {
          body = $('body');
        }
        window.setTimeout(function() {
          $.each(terms, function() {
            body.highlightText(this.toLowerCase(), 'highlighted');
          });
        }, 10);
        $('<p class="highlight-link"><a href="javascript:Documentation.' +
          'hideSearchWords()">' + _('Hide All')
          + '<span class="tag is-delete"></span>'
          + '</a></p>')
            .appendTo($('#searchbox'));
      }
    };
  });
  </script>
</div>
      
      <div class="sidebar-menu-toc">
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installing tensorly-Torch</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api.html#factorized-tensors">Factorized Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#tensorized-matrices">Tensorized Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#complex-tensors">Complex Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#module-tltorch.factorized_tensors">Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#module-tltorch.factorized_layers">Tensor Regression Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#tensor-contraction-layers">Tensor Contraction Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#factorized-linear-layers">Factorized Linear Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#factorized-convolutions">Factorized Convolutions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api.html#factorized-embeddings">Factorized Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#module-tltorch.tensor_hooks">Tensor Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#l1-regularization">L1 Regularization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#utilities">Utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../user_guide/index.html">User guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev_guide/index.html">Development guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About Us</a></li>
</ul>
 
      </div>
    </aside>
  </div>
  

  <div class="column main-column">

    
    <div class="main-section">

      
      
      <div class="side-menu-toggle">
        <button class="button" id="toggle-sidebar" onclick="toggle_sidebar()">
          <span class="icon"><i class="fa fa-bars" aria-hidden="true"></i></span>
          <span>menu</span> 
        </button>
      </div>
      

      <div class="container content main-content">
        
  <section id="tltorch-factorized-layers-factorizedembedding">
<h1><a class="reference internal" href="../api.html#module-tltorch.factorized_layers" title="tltorch.factorized_layers"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tltorch.factorized_layers</span></code></a>.FactorizedEmbedding</h1>
<dl class="py class">
<dt class="sig sig-object py" id="tltorch.factorized_layers.FactorizedEmbedding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">tltorch.factorized_layers.</span></span><span class="sig-name descname"><span class="pre">FactorizedEmbedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_embeddings</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_tensorize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_tensorized_modes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensorized_num_embeddings</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensorized_embedding_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factorization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'blocktt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tltorch/factorized_layers/factorized_embedding.html#FactorizedEmbedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Tensorized Embedding Layers For Efficient Model Compression
Tensorized drop-in replacement for <cite>torch.nn.Embedding</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>num_embeddings</strong><span class="classifier">int</span></dt><dd><p>number of entries in the lookup table</p>
</dd>
<dt><strong>embedding_dim</strong><span class="classifier">int</span></dt><dd><p>number of dimensions per entry</p>
</dd>
<dt><strong>auto_tensorize</strong><span class="classifier">bool</span></dt><dd><p>whether to use automatic reshaping for the embedding dimensions</p>
</dd>
<dt><strong>n_tensorized_modes</strong><span class="classifier">int or int tuple</span></dt><dd><p>number of reshape dimensions for both embedding table dimension</p>
</dd>
<dt><strong>tensorized_num_embeddings</strong><span class="classifier">int tuple</span></dt><dd><p>tensorized shape of the first embedding table dimension</p>
</dd>
<dt><strong>tensorized_embedding_dim</strong><span class="classifier">int tuple</span></dt><dd><p>tensorized shape of the second embedding table dimension</p>
</dd>
<dt><strong>factorization</strong><span class="classifier">str</span></dt><dd><p>tensor type</p>
</dd>
<dt><strong>rank</strong><span class="classifier">int tuple or str</span></dt><dd><p>rank of the tensor factorization</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#tltorch.factorized_layers.FactorizedEmbedding.forward" title="tltorch.factorized_layers.FactorizedEmbedding.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(input[, indices])</p></td>
<td><p>Defines the computation performed at every call.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#tltorch.factorized_layers.FactorizedEmbedding.from_embedding" title="tltorch.factorized_layers.FactorizedEmbedding.from_embedding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_embedding</span></code></a>(embedding_layer[, rank, ...])</p></td>
<td><p>Create a tensorized embedding layer from a regular embedding layer</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#tltorch.factorized_layers.FactorizedEmbedding.from_embedding_list" title="tltorch.factorized_layers.FactorizedEmbedding.from_embedding_list"><code class="xref py py-obj docutils literal notranslate"><span class="pre">from_embedding_list</span></code></a>(embedding_layer_list[, ...])</p></td>
<td><p>Create a tensorized embedding layer from a regular embedding layer</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><strong>get_embedding</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>reset_parameters</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="tltorch.factorized_layers.FactorizedEmbedding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tltorch/factorized_layers/factorized_embedding.html#FactorizedEmbedding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tltorch.factorized_layers.FactorizedEmbedding.from_embedding">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factorization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'blocktt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_tensorized_modes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decompose_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_tensorize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decomposition_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tltorch/factorized_layers/factorized_embedding.html#FactorizedEmbedding.from_embedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create a tensorized embedding layer from a regular embedding layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>embedding_layer</strong><span class="classifier">torch.nn.Embedding</span></dt><dd></dd>
<dt><strong>rank</strong><span class="classifier">int tuple or str</span></dt><dd><p>rank of the tensor decomposition</p>
</dd>
<dt><strong>factorization</strong><span class="classifier">str</span></dt><dd><p>tensor type</p>
</dd>
<dt><strong>decompose_weights: bool</strong></dt><dd><p>whether to decompose weights and use for initialization</p>
</dd>
<dt><strong>auto_tensorize: bool</strong></dt><dd><p>if True, automatically reshape dimensions for TensorizedTensor</p>
</dd>
<dt><strong>decomposition_kwargs: dict</strong></dt><dd><p>specify kwargs for the decomposition</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="tltorch.factorized_layers.FactorizedEmbedding.from_embedding_list">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_embedding_list</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_layer_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factorization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'blocktt'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_tensorized_modes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decompose_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_tensorize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decomposition_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{}</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tltorch/factorized_layers/factorized_embedding.html#FactorizedEmbedding.from_embedding_list"><span class="viewcode-link"><span class="pre">[source]</span></span></a></dt>
<dd><p>Create a tensorized embedding layer from a regular embedding layer</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>embedding_layer</strong><span class="classifier">torch.nn.Embedding</span></dt><dd></dd>
<dt><strong>rank</strong><span class="classifier">int tuple or str</span></dt><dd><p>tensor rank</p>
</dd>
<dt><strong>factorization</strong><span class="classifier">str</span></dt><dd><p>tensor decomposition to use</p>
</dd>
<dt><strong>decompose_weights: bool</strong></dt><dd><p>decompose weights and use for initialization</p>
</dd>
<dt><strong>auto_tensorize: bool</strong></dt><dd><p>automatically reshape dimensions for TensorizedTensor</p>
</dd>
<dt><strong>decomposition_kwargs: dict</strong></dt><dd><p>specify kwargs for the decomposition</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<div class="clearer"></div></section>


      </div>

      
        <nav class="pagination" role="navigation" aria-label="pagination">
    
    <a class="button pagination-previous" href="tltorch.factorized_layers.FactorizedConv.html" title="previous page" accesskey="p">
        <span class="icon">
            <i class="fa fa-arrow-circle-left"></i>
        </span>
        <span><code class="xref py py-mod docutils literal notranslate"><span class="pre">tltorch.factorized_layers</span></code>.FactorizedConv</span>
    </a>
    
    
    <a class="button pagination-next" href="tltorch.tensor_hooks.tensor_dropout.html" title="next page" accesskey="n">
        <span><code class="xref py py-mod docutils literal notranslate"><span class="pre">tltorch.tensor_hooks</span></code>.tensor_dropout </span>
        <span class="icon">
            <i class="fa fa-arrow-circle-right"></i>
        </span>
    </a>
    
</nav>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2022, Jean Kossaifi.<br/>
        </div>
      <div class="block">
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> and the <a href="http://tensorly.org"><strong>TensorLy</strong></a> theme by <a href="http://jeankossaifi.com">Jean Kossaifi</a>.
      </div>
    </div>
  </footer>

    </div>

  </div>  

	
    
    <div class="column is-hidden-touch is-2-desktop is-one-fifth-widescreen" id="localtoc-column">

    <aside class="sticky-nav localtoc"> 
        <p class="menu-label"> 
            <span class="icon-text">
                <span class="icon"><i class="fas fa-duotone fa-list"></i></span>
                <span> On this page </span>
            </span>
        </p>

        <div class="menu menu-list localtoc-list">
        <ul>
<li><a class="reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tltorch.factorized_layers</span></code>.FactorizedEmbedding</a><ul>
<li><a class="reference internal" href="#tltorch.factorized_layers.FactorizedEmbedding"><code class="docutils literal notranslate"><span class="pre">FactorizedEmbedding</span></code></a><ul>
<li><a class="reference internal" href="#tltorch.factorized_layers.FactorizedEmbedding.forward"><code class="docutils literal notranslate"><span class="pre">FactorizedEmbedding.forward()</span></code></a></li>
<li><a class="reference internal" href="#tltorch.factorized_layers.FactorizedEmbedding.from_embedding"><code class="docutils literal notranslate"><span class="pre">FactorizedEmbedding.from_embedding()</span></code></a></li>
<li><a class="reference internal" href="#tltorch.factorized_layers.FactorizedEmbedding.from_embedding_list"><code class="docutils literal notranslate"><span class="pre">FactorizedEmbedding.from_embedding_list()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
    </aside>
    </div>

  

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>